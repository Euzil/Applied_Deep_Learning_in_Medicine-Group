
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-06-05 23:07:54.695687: Using torch.compile... 
2025-06-05 23:07:56.547275: do_dummy_2d_data_aug: False 
2025-06-05 23:07:56.553886: Using splits from existing split file: /u/home/wyou/Documents/nnUNet/nnUNetFrame/dataset_221/nnUNet_preprocessed/Dataset221_KiTS2023/splits_final.json 
2025-06-05 23:07:56.567285: The split file contains 5 splits. 
2025-06-05 23:07:56.571694: Desired fold for training: 1 
2025-06-05 23:07:56.576618: This split has 321 training and 80 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [528.0, 512.0, 512.0], 'spacing': [0.78126, 0.78125, 0.78125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset221_KiTS2023', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.78125, 0.78125], 'original_median_shape_after_transp': [103, 512, 512], 'image_reader_writer': 'NibabelIOWithReorient', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 103.88595581054688, 'median': 103.0, 'min': -1021.0, 'percentile_00_5': -57.0, 'percentile_99_5': 302.0, 'std': 72.35579681396484}}} 
 
2025-06-05 23:08:02.406250: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-06-05 23:08:02.662738:  
2025-06-05 23:08:02.706421: Epoch 0 
2025-06-05 23:08:02.746821: Current learning rate: 0.01 
2025-06-05 23:16:16.986794: train_loss 0.0897 
2025-06-05 23:16:17.014034: val_loss -0.0011 
2025-06-05 23:16:17.016220: Pseudo dice [np.float64(0.4822), np.float64(0.0), np.float64(0.0)] 
2025-06-05 23:16:17.021389: Epoch time: 494.33 s 
2025-06-05 23:16:17.024284: Yayy! New best EMA pseudo Dice: 0.1607 
2025-06-05 23:17:19.095315:  
2025-06-05 23:17:19.096432: Epoch 1 
2025-06-05 23:17:19.097686: Current learning rate: 0.00998 
2025-06-05 23:20:55.625401: train_loss -0.0568 
2025-06-05 23:20:55.645343: val_loss -0.0793 
2025-06-05 23:20:55.646449: Pseudo dice [np.float64(0.6505), np.float64(0.0), np.float64(0.0)] 
2025-06-05 23:20:55.650460: Epoch time: 216.53 s 
2025-06-05 23:20:55.651550: Yayy! New best EMA pseudo Dice: 0.1664 
2025-06-05 23:20:59.127973:  
2025-06-05 23:20:59.129045: Epoch 2 
2025-06-05 23:20:59.129785: Current learning rate: 0.00996 
2025-06-05 23:22:03.821553: train_loss -0.1025 
2025-06-05 23:22:03.828337: val_loss -0.1469 
2025-06-05 23:22:03.829757: Pseudo dice [np.float64(0.6909), np.float64(0.0), np.float64(0.0)] 
2025-06-05 23:22:03.834201: Epoch time: 64.69 s 
2025-06-05 23:22:03.837095: Yayy! New best EMA pseudo Dice: 0.1727 
2025-06-05 23:28:10.991096:  
2025-06-05 23:28:10.992768: Epoch 3 
2025-06-05 23:28:10.993570: Current learning rate: 0.00995 
2025-06-05 23:29:15.604403: train_loss -0.122 
2025-06-05 23:29:15.612325: val_loss -0.1741 
2025-06-05 23:29:15.614128: Pseudo dice [np.float64(0.7213), np.float64(0.0), np.float64(0.0035)] 
2025-06-05 23:29:15.616991: Epoch time: 64.61 s 
2025-06-05 23:29:15.621667: Yayy! New best EMA pseudo Dice: 0.1796 
2025-06-05 23:30:08.240415:  
2025-06-05 23:30:08.241282: Epoch 4 
2025-06-05 23:30:08.242336: Current learning rate: 0.00993 
2025-06-05 23:37:36.495779: train_loss -0.1794 
2025-06-05 23:37:43.423162: val_loss -0.2013 
2025-06-05 23:37:43.424328: Pseudo dice [np.float64(0.7373), np.float64(0.3773), np.float64(0.3752)] 
2025-06-05 23:37:43.425634: Epoch time: 448.26 s 
2025-06-05 23:37:43.426872: Yayy! New best EMA pseudo Dice: 0.2113 
2025-06-05 23:37:46.788394:  
2025-06-05 23:37:46.789346: Epoch 5 
2025-06-05 23:37:46.790426: Current learning rate: 0.00991 
2025-06-05 23:38:51.738498: train_loss -0.1929 
2025-06-05 23:38:51.742768: val_loss -0.1906 
2025-06-05 23:38:51.743608: Pseudo dice [np.float64(0.7541), np.float64(0.4353), np.float64(0.398)] 
2025-06-05 23:38:51.744446: Epoch time: 64.95 s 
2025-06-05 23:38:51.745308: Yayy! New best EMA pseudo Dice: 0.2431 
2025-06-05 23:42:14.783161:  
2025-06-05 23:42:14.784050: Epoch 6 
2025-06-05 23:42:14.784860: Current learning rate: 0.00989 
2025-06-05 23:44:14.708312: train_loss -0.2284 
2025-06-05 23:44:14.773170: val_loss -0.2104 
2025-06-05 23:44:14.790940: Pseudo dice [np.float64(0.761), np.float64(0.3775), np.float64(0.3046)] 
2025-06-05 23:44:14.804479: Epoch time: 119.93 s 
2025-06-05 23:44:14.818382: Yayy! New best EMA pseudo Dice: 0.2669 
2025-06-05 23:44:18.245065:  
2025-06-05 23:44:18.251876: Epoch 7 
2025-06-05 23:44:18.259794: Current learning rate: 0.00987 
2025-06-05 23:53:05.166404: train_loss -0.2008 
2025-06-05 23:53:05.188819: val_loss -0.2185 
2025-06-05 23:53:05.194884: Pseudo dice [np.float64(0.8019), np.float64(0.2861), np.float64(0.2471)] 
2025-06-05 23:53:05.198652: Epoch time: 526.92 s 
2025-06-05 23:53:05.201571: Yayy! New best EMA pseudo Dice: 0.2847 
2025-06-05 23:55:19.368629:  
2025-06-05 23:55:19.369793: Epoch 8 
2025-06-05 23:55:19.370800: Current learning rate: 0.00986 
2025-06-05 23:57:02.506031: train_loss -0.2508 
2025-06-05 23:57:02.511601: val_loss -0.2883 
2025-06-05 23:57:02.513597: Pseudo dice [np.float64(0.8248), np.float64(0.481), np.float64(0.472)] 
2025-06-05 23:57:02.515593: Epoch time: 103.14 s 
2025-06-05 23:57:02.518976: Yayy! New best EMA pseudo Dice: 0.3155 
2025-06-05 23:58:54.700666:  
2025-06-05 23:58:54.701727: Epoch 9 
2025-06-05 23:58:54.702840: Current learning rate: 0.00984 
2025-06-06 00:05:46.606993: train_loss -0.2546 
2025-06-06 00:05:46.613309: val_loss -0.3282 
2025-06-06 00:05:46.614828: Pseudo dice [np.float64(0.8298), np.float64(0.5429), np.float64(0.5006)] 
2025-06-06 00:05:46.616009: Epoch time: 411.91 s 
2025-06-06 00:05:46.617136: Yayy! New best EMA pseudo Dice: 0.3464 
2025-06-06 00:05:49.992333:  
2025-06-06 00:05:49.993037: Epoch 10 
2025-06-06 00:05:49.994035: Current learning rate: 0.00982 
2025-06-06 00:06:55.178524: train_loss -0.231 
2025-06-06 00:06:55.189090: val_loss -0.2453 
2025-06-06 00:06:55.190605: Pseudo dice [np.float64(0.7892), np.float64(0.3736), np.float64(0.3535)] 
2025-06-06 00:06:55.192654: Epoch time: 65.19 s 
2025-06-06 00:06:55.195444: Yayy! New best EMA pseudo Dice: 0.3623 
2025-06-06 00:10:28.497518:  
2025-06-06 00:10:28.499271: Epoch 11 
2025-06-06 00:10:28.500331: Current learning rate: 0.0098 
